{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Neural networks with Keras\n",
    "\n",
    "On to the most exciting machine learning technique: Neural networks. as you will see they are just as easy to use as the sklearn methods.\n",
    "\n",
    "There are several deep learning libraries for Python. The three most commonly used are:\n",
    "\n",
    "- Keras: High level library based on Tensorflow (or others) that is easy to use and flexible enough for most standard users. It has a great documentation and online support.\n",
    "- Tensorflow: Google's neural network library. Most widely used in ML research. Flexible and powerful but also (unnecessarily?) complicated.\n",
    "- Pytorch: The newcomer developed by Facebook. Flexible like Tensorflow but with a nicer, more Pythonic API.\n",
    "\n",
    "Here we will use Keras which is a great start for most tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/local/S.Rasp/ML-Workshop-Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(preds, fn=None):\n",
    "    df =  pd.DataFrame({'Id': range(len(preds)), 'Expected': preds})\n",
    "    if fn is not None: df.to_csv(DATADIR + fn, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('./data/preproc_data.pkl', 'rb') as f:\n",
    "    X_train, y_train, X_valid, y_valid, X_test = pickle.load(f)\n",
    "with open('./data/dfs.pkl', 'rb') as f:\n",
    "    df_train, df_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: The Keras way\n",
    "\n",
    "First, let's build our own linear regression algorithm with stochastic gradient descent. This should give us the same solution as the sklearn linear regression we did in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728008, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to build a model in Keras. We will start with the easier, a Sequential model. This means that it is a succession of layers. For linear regression we only have one linear layer that maps the inputs to the outputs. Layers where all inputs are connected to all outputs are called fully-connected or Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = Sequential([Dense(1, input_shape=(22,), activation='linear')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 23\n",
      "Trainable params: 23\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ln.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has 23 parameters, 22 coefficients plus one bias term.\n",
    "\n",
    "Next we need to compile the model, which basically means telling Keras which optimizer to use and which loss function to minimize. In the background it also randomly initializes the weights and biases at this stage.\n",
    "\n",
    "We will use the Adam optimizer, which is a fancy version of SGD: https://arxiv.org/abs/1412.6980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.compile(Adam(1e-1), 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train/fit the model. For this we specify the training data, the batch size, the number of epochs. Optionally, we can pass on the validation data, so that we get a validation score every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 728008 samples, validate on 180849 samples\n",
      "Epoch 1/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 27.4319 - val_loss: 6.8245\n",
      "Epoch 2/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.8454 - val_loss: 3.5604\n",
      "Epoch 3/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.1538 - val_loss: 3.4450\n",
      "Epoch 4/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0922 - val_loss: 3.3622\n",
      "Epoch 5/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0536 - val_loss: 3.3181\n",
      "Epoch 6/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0309 - val_loss: 3.2888\n",
      "Epoch 7/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0183 - val_loss: 3.2647\n",
      "Epoch 8/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0108 - val_loss: 3.2571\n",
      "Epoch 9/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0076 - val_loss: 3.2457\n",
      "Epoch 10/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0105 - val_loss: 3.2460\n",
      "Epoch 11/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0086 - val_loss: 3.2362\n",
      "Epoch 12/12\n",
      "728008/728008 [==============================] - 1s 1us/step - loss: 3.0055 - val_loss: 3.2519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ffc5d7f28>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln.fit(X_train, y_train, 10_000, epochs=12, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180849/180849 [==============================] - 0s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2519269126155113"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln.evaluate(X_valid, y_valid, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that with the sklearn linear regression algorithm we got a score of around 3.24. This indicates that we are doing pretty much the same here. But we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with a hidden layer\n",
    "\n",
    "Now let's actually build a neural network with a hidden layer. For this we simply add another Dense layer but this time with a non-linear activation function. In our case this is a Rectified Linear Unit or relu. There is no set rule for how many hidden layers or nodes to use. For this we just need to employ trial-and-error.\n",
    "\n",
    "BTW: You will often see people using powers of two for the batch size or the number of nodes. This is for optimization purposes on the GPU, which are not crucial in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential([\n",
    "    Dense(256, input_shape=(22,), activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 256)               5888      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,145\n",
      "Trainable params: 6,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(Adam(1e-3), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 728008 samples, validate on 180849 samples\n",
      "Epoch 1/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 8.6937 - val_loss: 3.3594\n",
      "Epoch 2/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.8275 - val_loss: 3.0153\n",
      "Epoch 3/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.6569 - val_loss: 2.9774\n",
      "Epoch 4/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.5852 - val_loss: 2.9399\n",
      "Epoch 5/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.5366 - val_loss: 2.9111\n",
      "Epoch 6/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.4968 - val_loss: 2.9116\n",
      "Epoch 7/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.4645 - val_loss: 2.9114\n",
      "Epoch 8/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.4344 - val_loss: 2.9096\n",
      "Epoch 9/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.4088 - val_loss: 2.8838\n",
      "Epoch 10/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.3885 - val_loss: 2.8902\n",
      "Epoch 11/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.3682 - val_loss: 2.8726\n",
      "Epoch 12/12\n",
      "728008/728008 [==============================] - 4s 6us/step - loss: 2.3490 - val_loss: 2.8944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0dabd17fd0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, 1024, epochs=12, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some notes on the results*\n",
    "\n",
    "- We see that we are quite heavily overfitting. This makes sense in a model with 6k parameters. There are techniques to prevent overfitting in neural networks, most notably dropout and weight decay (L2 regularization). We will cover those later. For some reason, I found those techniques not to work for this model and dataset. Can you figure out why?\n",
    "- The score is quite significantly better than our best single random forest model. This suggests that the nonlinear computing power of the neural network is useful.\n",
    "- This is not yet a deep neural network. DNNs have several hidden layers. Again, I found that for this dataset a DNN didn't perform better. This might be because the nonlinearities are not very strong, but feel free to prove me wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANCED TECHNIQUE ALERT: Embeddings\n",
    "\n",
    "We saw in the previous notebook that the station information might be really important. We could train a separate neural network for each station but that would reduce the amount of training data for each individual model and probably lead to stronger overfitting. \n",
    "\n",
    "Here is a different method of using categorical variables in neural networks. Don't worry if you don't understand embeddings right away. The concept is easy but it takes a while to wrap your head around it (it did for me anyways). \n",
    "\n",
    "First we need to get continuous station IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2015-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = df_train.station\n",
    "stations_train = df_train.station[df_train.time < split_date]\n",
    "stations_valid = df_train.station[df_train.time >= split_date]\n",
    "stations_test = df_test.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations = pd.concat([df_train.station, df_test.station]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat2id = {s: i for i, s in enumerate(unique_stations)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = stations.apply(lambda x: stat2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = ids[df_train.time < split_date]\n",
    "ids_valid = ids[df_train.time >= split_date]\n",
    "ids_test = stations_test.apply(lambda x: stat2id[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have to use the Keras's Functional API. Check out the official documentation.\n",
    "\n",
    "We now have two separate inputs. Our regular features and the station ID.\n",
    "\n",
    "An embedding is a mapping from an integer to a vector of real numbers. In our case the vector has length two. The elements are also called latent features. These are then concatenates with the regular features and passed through one hidden layer as before.\n",
    "\n",
    "The latent features are updated along with the weights and biases during training and can now represent station-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in = Input(shape=(22,))\n",
    "id_in = Input(shape=(1,))\n",
    "emb = Embedding(len(unique_stations), 2)(id_in)\n",
    "emb = Flatten()(emb)\n",
    "x = Concatenate()([features_in, emb])\n",
    "x = Dense(100, activation='relu')(x)\n",
    "out = Dense(1, activation='linear')(x)\n",
    "model = Model(inputs=[features_in, id_in], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 2)         1044        input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2)            0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 24)           0           input_19[0][0]                   \n",
      "                                                                 flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 100)          2500        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 1)            101         dense_45[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,645\n",
      "Trainable params: 3,645\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 728008 samples, validate on 180849 samples\n",
      "Epoch 1/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 10.0610 - val_loss: 3.1637\n",
      "Epoch 2/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.6669 - val_loss: 2.8119\n",
      "Epoch 3/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.4447 - val_loss: 2.6857\n",
      "Epoch 4/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.3121 - val_loss: 2.5957\n",
      "Epoch 5/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.2150 - val_loss: 2.5539\n",
      "Epoch 6/10\n",
      "728008/728008 [==============================] - 5s 6us/step - loss: 2.1624 - val_loss: 2.5465\n",
      "Epoch 7/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.1284 - val_loss: 2.5363\n",
      "Epoch 8/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.0998 - val_loss: 2.5083\n",
      "Epoch 9/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.0756 - val_loss: 2.4812\n",
      "Epoch 10/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.0550 - val_loss: 2.4697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0daba62588>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train, ids_train], y_train, 1024, 10, \n",
    "          validation_data=([X_valid, ids_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 728008 samples, validate on 180849 samples\n",
      "Epoch 1/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.0365 - val_loss: 2.4749\n",
      "Epoch 2/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.0214 - val_loss: 2.4874\n",
      "Epoch 3/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 2.0089 - val_loss: 2.4873\n",
      "Epoch 4/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9965 - val_loss: 2.4698\n",
      "Epoch 5/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9862 - val_loss: 2.4793\n",
      "Epoch 6/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9777 - val_loss: 2.4782\n",
      "Epoch 7/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9689 - val_loss: 2.4486\n",
      "Epoch 8/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9615 - val_loss: 2.4525\n",
      "Epoch 9/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9541 - val_loss: 2.4520\n",
      "Epoch 10/10\n",
      "728008/728008 [==============================] - 5s 7us/step - loss: 1.9479 - val_loss: 2.4615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1052f68048>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train, ids_train], y_train, 1024, 10, \n",
    "          validation_data=([X_valid, ids_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.057278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.173167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.419005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.545274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.181433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Expected\n",
       "0   0  3.057278\n",
       "1   1  2.173167\n",
       "2   2  0.419005\n",
       "3   3  2.545274\n",
       "4   4  2.181433"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit to Kaggle\n",
    "df_sub = create_sub(model.predict([X_test, ids_test]).squeeze(), 'nn_emb.csv'); df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique allows us to build a single model that incorporates station information and gives us the best score. Yay!\n",
    "\n",
    "But can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "1. As we have seen now there are a lot of hyperparameters. Try playing around with them and get the best score. How do the parameters influence the skill?\n",
    "2. Try an ensemble of techniques. This means training a few models (can be several NNs or some NNs with some RFs) and averaging the predictions. This is also a way to prevent overfitting and might just increase your score ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
